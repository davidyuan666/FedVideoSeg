"""
åŸºäºäºŒåˆ†æ³•æœç´¢çš„è§†é¢‘å…³é”®å¸§æå–å™¨
ä½¿ç”¨CLIPå’ŒDeepSeekè¿›è¡Œæ™ºèƒ½å¸§é€‰æ‹©
"""

import torch
import cv2
import os
import json
import time
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Any
import numpy as np
from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
from dotenv import load_dotenv

load_dotenv()

class DeepSeekClient:
    """DeepSeek APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str = None, base_url: str = "https://api.deepseek.com/v1"):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        if not self.api_key:
            logger.warning("DeepSeek API keyæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
    
    def evaluate_relevance(self, frame_description: str, question: str) -> float:
        """è¯„ä¼°å¸§æè¿°ä¸é—®é¢˜çš„ç›¸å…³æ€§ (0-1)"""
        if not self.api_key:
            # æ¨¡æ‹Ÿç›¸å…³æ€§è¯„åˆ†
            keywords = question.lower().split()
            desc_lower = frame_description.lower()
            matches = sum(1 for word in keywords if word in desc_lower)
            return min(0.9, matches / len(keywords) * 0.8 + 0.1)
        
        prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹è§†é¢‘å¸§æè¿°ä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼Œè¿”å›0-1ä¹‹é—´çš„åˆ†æ•°ã€‚
        1.0è¡¨ç¤ºå®Œå…¨ç›¸å…³ï¼Œ0.0è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ã€‚
        
        é—®é¢˜ï¼š{question}
        å¸§æè¿°ï¼š{frame_description}
        
        è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼š
        """
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json={
                    "model": "deepseek-chat",
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 10,
                    "temperature": 0.1
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                score_text = result["choices"][0]["message"]["content"].strip()
                try:
                    score = float(score_text)
                    return max(0.0, min(1.0, score))
                except ValueError:
                    return 0.5
            else:
                logger.error(f"DeepSeek APIé”™è¯¯: {response.status_code}")
                return 0.5
                
        except Exception as e:
            logger.error(f"è¯„ä¼°ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.5

class CLIPFrameAnalyzer:
    """ä½¿ç”¨CLIPå’Œå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹åˆ†æè§†é¢‘å¸§"""
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = CLIPModel.from_pretrained(model_name, cache_dir='cache')
        self.processor = CLIPProcessor.from_pretrained(model_name, cache_dir='cache')
        self.model.to(self.device)
        
        # åˆå§‹åŒ–å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹
        try:
            from transformers import BlipProcessor, BlipForConditionalGeneration
            self.caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base", cache_dir='cache')
            self.caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", cache_dir='cache')
            self.caption_model.to(self.device)
            self.use_blip = True
            logger.info("BLIPå›¾åƒæè¿°æ¨¡å‹å·²åŠ è½½")
        except ImportError:
            logger.warning("BLIPæ¨¡å‹ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¢å¼ºçš„CLIPåˆ†æ")
            self.use_blip = False
        
        logger.info(f"CLIPæ¨¡å‹å·²åŠ è½½: {model_name}")
    
    def describe_frame_with_blip(self, frame: np.ndarray) -> str:
        """ä½¿ç”¨BLIPæ¨¡å‹ç”Ÿæˆå¸§çš„å¼€æ”¾å¼æè¿°"""
        try:
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if isinstance(frame, np.ndarray):
                frame = Image.fromarray(frame)
            
            # ä½¿ç”¨BLIPç”Ÿæˆæè¿°
            inputs = self.caption_processor(frame, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                out = self.caption_model.generate(**inputs, max_length=50, num_beams=5)
            
            caption = self.caption_processor.decode(out[0], skip_special_tokens=True)
            
            logger.debug(f"BLIPå¸§æè¿°: {caption}")
            return caption
            
        except Exception as e:
            logger.error(f"BLIPåˆ†æå¸§å¤±è´¥: {e}")
            return self._fallback_describe(frame)
    
    def _fallback_describe(self, frame: np.ndarray) -> str:
        """å¢å¼ºçš„CLIPæè¿°æ–¹æ³•ï¼šä½¿ç”¨æ›´è¯¦ç»†çš„åœºæ™¯åˆ†æ"""
        try:
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if isinstance(frame, np.ndarray):
                frame = Image.fromarray(frame)
            
            # ä½¿ç”¨æ›´è¯¦ç»†å’Œå…·ä½“çš„æè¿°æ¨¡æ¿
            detailed_descriptions = [
                "a person cooking food in a kitchen with pots and pans",
                "people eating a meal together at a dining table", 
                "someone talking on a mobile phone or device",
                "people having a conversation in a living room",
                "a person working at a desk with a computer",
                "people watching television in a comfortable room",
                "someone cleaning or organizing household items",
                "people exercising or doing physical activities",
                "a person reading a book or studying materials",
                "people playing games or socializing together",
                "outdoor activities in a natural environment",
                "street scenes with cars and pedestrians",
                "shopping activities in a store or market",
                "business meeting or educational discussion",
                "family gathering or celebration event",
                "kitchen activities with food preparation",
                "bedroom or private room activities",
                "bathroom or personal care activities",
                "garden or outdoor home activities",
                "workshop or garage activities"
            ]
            
            # å¤„ç†å›¾åƒå’Œæ–‡æœ¬
            inputs = self.processor(
                text=detailed_descriptions,
                images=frame,
                return_tensors="pt",
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=-1)
            
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„æè¿°
            best_idx = probs.argmax().item()
            confidence = probs[0][best_idx].item()
            
            best_description = detailed_descriptions[best_idx]
            
            # å¦‚æœç½®ä¿¡åº¦è¾ƒä½ï¼Œå°è¯•ç»„åˆæè¿°
            if confidence < 0.15:
                # è·å–å‰3ä¸ªå€™é€‰å¹¶ç»„åˆ
                top_k = 3
                top_indices = probs[0].topk(top_k).indices
                top_probs = probs[0].topk(top_k).values
                
                description_parts = []
                for idx, prob in zip(top_indices, top_probs):
                    if prob > 0.08:  # è¾ƒä½çš„é˜ˆå€¼
                        desc = detailed_descriptions[idx]
                        # æå–å…³é”®è¯
                        key_words = desc.split()[:3]  # å–å‰3ä¸ªè¯ä½œä¸ºå…³é”®æè¿°
                        description_parts.extend(key_words)
                
                if description_parts:
                    best_description = " ".join(description_parts[:5])  # ç»„åˆå‰5ä¸ªå…³é”®è¯
                else:
                    best_description = "indoor daily life scene"
            
            logger.debug(f"å¢å¼ºCLIPå¸§æè¿°: {best_description} (ç½®ä¿¡åº¦: {confidence:.3f})")
            return best_description
                
        except Exception as e:
            logger.error(f"å¢å¼ºCLIPåˆ†æå¸§å¤±è´¥: {e}")
            return "video scene content"
    
    def describe_frame(self, frame: np.ndarray, deepseek_client=None) -> str:
        """ç”Ÿæˆå¸§çš„å¼€æ”¾å¼æè¿°"""
        # ä¼˜å…ˆä½¿ç”¨BLIPæ¨¡å‹è¿›è¡Œå¼€æ”¾å¼æè¿°
        if self.use_blip:
            return self.describe_frame_with_blip(frame)
        # ä½¿ç”¨å¢å¼ºçš„CLIPåˆ†æ
        else:
            return self._fallback_describe(frame)

class BinarySearchExtractor:
    """äºŒåˆ†æ³•è§†é¢‘å…³é”®å¸§æå–å™¨"""
    
    def __init__(self, deepseek_api_key: str = None):
        self.clip_analyzer = CLIPFrameAnalyzer()
        self.deepseek_client = DeepSeekClient(deepseek_api_key)
        self.min_segment_duration = 2.0  # æœ€å°ç‰‡æ®µæ—¶é•¿(ç§’)
        self.max_frames = 5  # æœ€å¤§æå–å¸§æ•°
        self.initial_segment_duration = 5.0  # åˆæ­¥ç­›é€‰çš„ç‰‡æ®µæ—¶é•¿(ç§’)
        self.top_segments_for_binary_search = 3  # é€‰æ‹©å‰Nä¸ªç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•æœç´¢
        
        # é¢„å®šä¹‰é—®é¢˜
        self.predefined_questions = [
            "when does the man boil milk",
            "when does the man make cake"
        ]
    
    def extract_frame_at_time(self, video_path: str, timestamp: float) -> np.ndarray:
        """åœ¨æŒ‡å®šæ—¶é—´æå–è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_number = int(timestamp * fps)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
        
        ret, frame = cap.read()
        cap.release()
        
        if ret:
            # è½¬æ¢BGRåˆ°RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            return frame
        else:
            raise ValueError(f"æ— æ³•åœ¨æ—¶é—´æˆ³ {timestamp} æå–å¸§")
    
    def get_video_duration(self, video_path: str) -> float:
        """è·å–è§†é¢‘æ—¶é•¿"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        duration = frame_count / fps if fps > 0 else 0
        cap.release()
        
        return duration
    
    def evaluate_segment_relevance(self, video_path: str, start_time: float, 
                                 end_time: float, question: str) -> float:
        """è¯„ä¼°è§†é¢‘ç‰‡æ®µä¸é—®é¢˜çš„ç›¸å…³æ€§"""
        try:
            logger.info(f"   ğŸ–¼ï¸  æå–ç‰‡æ®µé¦–å°¾å¸§: [{start_time:.1f}s] å’Œ [{end_time:.1f}s]")
            
            # æå–ç‰‡æ®µé¦–å°¾å¸§
            start_frame = self.extract_frame_at_time(video_path, start_time)
            end_frame = self.extract_frame_at_time(video_path, end_time)
            
            # è·å–å¸§æè¿°ï¼ˆä¼ å…¥deepseek_clientä»¥æ”¯æŒæ›´å¥½çš„æè¿°ï¼‰
            start_desc = self.clip_analyzer.describe_frame(start_frame, self.deepseek_client)
            end_desc = self.clip_analyzer.describe_frame(end_frame, self.deepseek_client)
            
            logger.info(f"   ğŸ“ å¸§æè¿° - å¼€å§‹å¸§: {start_desc}")
            logger.info(f"   ğŸ“ å¸§æè¿° - ç»“æŸå¸§: {end_desc}")
            
            # ç»„åˆæè¿°
            combined_desc = f"ç‰‡æ®µå¼€å§‹: {start_desc}, ç‰‡æ®µç»“æŸ: {end_desc}"
            
            # è¯„ä¼°ç›¸å…³æ€§
            logger.info(f"   ğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeekè¯„ä¼°ç›¸å…³æ€§...")
            relevance = self.deepseek_client.evaluate_relevance(combined_desc, question)
            
            logger.info(f"   â­ DeepSeekè¯„ä¼°ç»“æœ: {relevance:.3f}")
            
            return relevance
            
        except Exception as e:
            logger.error(f"   âŒ è¯„ä¼°ç‰‡æ®µç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0
    
    def initial_coarse_screening(self, video_path: str, question: str) -> List[Tuple[float, float, float]]:
        """åˆæ­¥ç²—ç­›é€‰ï¼šæ¯éš”5ç§’åˆ›å»ºç‰‡æ®µå¹¶è¯„ä¼°ç›¸å…³æ€§"""
        duration = self.get_video_duration(video_path)
        segments = []
        
        logger.info(f"ğŸ” å¼€å§‹åˆæ­¥ç²—ç­›é€‰ - è§†é¢‘æ€»æ—¶é•¿: {duration:.1f}s")
        logger.info(f"ğŸ“ ç‰‡æ®µé—´éš”: {self.initial_segment_duration}s")
        logger.info(f"ğŸ“‹ é—®é¢˜: {question}")
        
        # åˆ›å»ºæ¯éš”5ç§’çš„ç‰‡æ®µ
        current_time = 0
        segment_id = 0
        
        while current_time < duration:
            end_time = min(current_time + self.initial_segment_duration, duration)
            
            # ç¡®ä¿ç‰‡æ®µé•¿åº¦è¶³å¤Ÿ
            if end_time - current_time >= self.min_segment_duration:
                segment_id += 1
                
                logger.info(f"ğŸ¯ [ç²—ç­› {segment_id}] è¯„ä¼°ç‰‡æ®µ [{current_time:.1f}-{end_time:.1f}s] (æ—¶é•¿: {end_time-current_time:.1f}s)")
                
                # è¯„ä¼°ç‰‡æ®µç›¸å…³æ€§
                relevance = self.evaluate_segment_relevance(video_path, current_time, end_time, question)
                
                segments.append((current_time, end_time, relevance))
                
                logger.info(f"ğŸ“Š [ç²—ç­› {segment_id}] ç‰‡æ®µ [{current_time:.1f}-{end_time:.1f}s] ç›¸å…³æ€§åˆ†æ•°: {relevance:.3f}")
                
                # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
                time.sleep(0.3)
            
            current_time += self.initial_segment_duration
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        segments.sort(key=lambda x: x[2], reverse=True)
        
        logger.info(f"ğŸ ç²—ç­›é€‰å®Œæˆï¼Œå…±è¯„ä¼° {len(segments)} ä¸ªç‰‡æ®µ")
        
        # æ˜¾ç¤ºæ‰€æœ‰ç‰‡æ®µçš„ç›¸å…³æ€§åˆ†æ•°
        if segments:
            logger.info(f"ğŸ“‹ æ‰€æœ‰ç‰‡æ®µç›¸å…³æ€§æ’åº:")
            for i, (start, end, score) in enumerate(segments):
                logger.info(f"   {i+1}. [{start:.1f}-{end:.1f}s] åˆ†æ•°: {score:.3f}")
        
        # é€‰æ‹©ç›¸å…³æ€§æœ€é«˜çš„å‰å‡ ä¸ªç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•æœç´¢
        top_segments = segments[:self.top_segments_for_binary_search]
        
        if top_segments:
            logger.info(f"ğŸ¯ é€‰æ‹©å‰ {len(top_segments)} ä¸ªç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•ç»†åŒ–æœç´¢:")
            for i, (start, end, score) in enumerate(top_segments):
                logger.info(f"   é€‰ä¸­ {i+1}. [{start:.1f}-{end:.1f}s] åˆ†æ•°: {score:.3f}")
        
        return top_segments

    def binary_search_on_segment(self, video_path: str, question: str, 
                                start_time: float, end_time: float) -> List[Tuple[float, float, float]]:
        """å¯¹æŒ‡å®šç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•æœç´¢"""
        segments = []  # (start_time, end_time, relevance_score)
        search_queue = [(start_time, end_time)]
        search_depth = 0
        
        logger.info(f"ğŸ”¬ å¯¹ç‰‡æ®µ [{start_time:.1f}-{end_time:.1f}s] å¼€å§‹äºŒåˆ†æ³•ç»†åŒ–æœç´¢")
        
        while search_queue and len(segments) < self.max_frames:
            current_start, current_end = search_queue.pop(0)
            search_depth += 1
            
            # å¦‚æœç‰‡æ®µå¤ªçŸ­ï¼Œè·³è¿‡
            if current_end - current_start < self.min_segment_duration:
                logger.info(f"â­ï¸  [äºŒåˆ† {search_depth}] ç‰‡æ®µ [{current_start:.1f}-{current_end:.1f}s] å¤ªçŸ­ï¼Œè·³è¿‡ (< {self.min_segment_duration}s)")
                continue
            
            logger.info(f"ğŸ¯ [äºŒåˆ† {search_depth}] è¯„ä¼°ç‰‡æ®µ [{current_start:.1f}-{current_end:.1f}s] (æ—¶é•¿: {current_end-current_start:.1f}s)")
            
            relevance = self.evaluate_segment_relevance(
                video_path, current_start, current_end, question
            )
            
            logger.info(f"ğŸ“Š [äºŒåˆ† {search_depth}] ç‰‡æ®µ [{current_start:.1f}-{current_end:.1f}s] ç›¸å…³æ€§åˆ†æ•°: {relevance:.3f}")
            
            # å¦‚æœç›¸å…³æ€§é«˜ï¼Œè®°å½•è¿™ä¸ªç‰‡æ®µ
            if relevance > 0.6:
                segments.append((current_start, current_end, relevance))
                logger.info(f"âœ… [äºŒåˆ† {search_depth}] é«˜ç›¸å…³æ€§ç‰‡æ®µå·²ä¿å­˜: [{current_start:.1f}-{current_end:.1f}s] (åˆ†æ•°: {relevance:.3f})")
            
            # å¦‚æœç›¸å…³æ€§ä¸­ç­‰ï¼Œç»§ç»­äºŒåˆ†æœç´¢
            elif relevance > 0.3:
                mid_time = (current_start + current_end) / 2
                
                # å°†ç‰‡æ®µåˆ†ä¸ºä¸¤åŠç»§ç»­æœç´¢
                search_queue.append((current_start, mid_time))
                search_queue.append((mid_time, current_end))
                
                logger.info(f"ğŸ”„ [äºŒåˆ† {search_depth}] ä¸­ç­‰ç›¸å…³æ€§ï¼Œç»§ç»­äºŒåˆ†æœç´¢:")
                logger.info(f"   â”œâ”€ å·¦åŠæ®µ: [{current_start:.1f}-{mid_time:.1f}s] (æ—¶é•¿: {mid_time-current_start:.1f}s)")
                logger.info(f"   â””â”€ å³åŠæ®µ: [{mid_time:.1f}-{current_end:.1f}s] (æ—¶é•¿: {current_end-mid_time:.1f}s)")
            
            else:
                logger.info(f"âŒ [äºŒåˆ† {search_depth}] ä½ç›¸å…³æ€§ç‰‡æ®µï¼Œåœæ­¢æœç´¢: [{current_start:.1f}-{current_end:.1f}s] (åˆ†æ•°: {relevance:.3f})")
            
            # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
            time.sleep(0.5)
        
        return segments

    def binary_search_segments(self, video_path: str, question: str, 
                             start_time: float = 0, end_time: float = None) -> List[Tuple[float, float, float]]:
        """æ”¹è¿›çš„äºŒåˆ†æ³•æœç´¢ï¼šå…ˆç²—ç­›é€‰ï¼Œå†å¯¹é€‰ä¸­çš„ç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•"""
        logger.info(f"ğŸš€ å¼€å§‹æ”¹è¿›çš„äºŒåˆ†æ³•æœç´¢æµç¨‹")
        
        # ç¬¬ä¸€æ­¥ï¼šåˆæ­¥ç²—ç­›é€‰
        top_segments = self.initial_coarse_screening(video_path, question)
        
        if not top_segments:
            logger.warning("ç²—ç­›é€‰æœªæ‰¾åˆ°ç›¸å…³ç‰‡æ®µ")
            return []
        
        # ç¬¬äºŒæ­¥ï¼šå¯¹é€‰ä¸­çš„ç‰‡æ®µè¿›è¡ŒäºŒåˆ†æ³•ç»†åŒ–æœç´¢
        all_refined_segments = []
        
        for i, (seg_start, seg_end, initial_score) in enumerate(top_segments):
            logger.info(f"\n--- äºŒåˆ†æ³•ç»†åŒ–æœç´¢ {i+1}/{len(top_segments)} ---")
            logger.info(f"ğŸ¯ ç›®æ ‡ç‰‡æ®µ: [{seg_start:.1f}-{seg_end:.1f}s] (åˆå§‹åˆ†æ•°: {initial_score:.3f})")
            
            refined_segments = self.binary_search_on_segment(
                video_path, question, seg_start, seg_end
            )
            
            # å¦‚æœäºŒåˆ†æ³•æ²¡æœ‰æ‰¾åˆ°æ›´å¥½çš„ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹ç‰‡æ®µ
            if not refined_segments and initial_score > 0.4:
                refined_segments = [(seg_start, seg_end, initial_score)]
                logger.info(f"ğŸ”„ äºŒåˆ†æ³•æœªæ‰¾åˆ°æ›´å¥½ç‰‡æ®µï¼Œä¿ç•™åŸå§‹ç‰‡æ®µ: [{seg_start:.1f}-{seg_end:.1f}s] (åˆ†æ•°: {initial_score:.3f})")
            
            all_refined_segments.extend(refined_segments)
            
            logger.info(f"âœ“ ç‰‡æ®µ {i+1} ç»†åŒ–å®Œæˆï¼Œæ‰¾åˆ° {len(refined_segments)} ä¸ªç²¾ç¡®ç‰‡æ®µ")
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ç»“æœå¹¶æ’åº
        all_refined_segments.sort(key=lambda x: x[2], reverse=True)
        
        # å»é‡ï¼šå¦‚æœæœ‰é‡å çš„ç‰‡æ®µï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„
        final_segments = []
        for seg in all_refined_segments:
            if len(final_segments) >= self.max_frames:
                break
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç‰‡æ®µé‡å 
            overlap = False
            for existing_seg in final_segments:
                if (seg[0] < existing_seg[1] and seg[1] > existing_seg[0]):  # æœ‰é‡å 
                    overlap = True
                    break
            
            if not overlap:
                final_segments.append(seg)
        
        logger.info(f"ğŸ æ”¹è¿›çš„äºŒåˆ†æ³•æœç´¢å®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(final_segments)} ä¸ªç‰‡æ®µ")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        if final_segments:
            logger.info(f"ğŸ“‹ æœ€ç»ˆé€‰æ‹©çš„ç‰‡æ®µ (æŒ‰ç›¸å…³æ€§æ’åº):")
            for i, (start, end, score) in enumerate(final_segments):
                logger.info(f"   {i+1}. [{start:.1f}-{end:.1f}s] åˆ†æ•°: {score:.3f}")
        
        return final_segments
    
    def extract_key_frames(self, video_path: str, question: str, output_dir: str) -> List[Dict]:
        """æå–å…³é”®å¸§å¹¶ä¿å­˜"""
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        logger.info(f"é—®é¢˜: {question}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æœç´¢ç›¸å…³ç‰‡æ®µ
        relevant_segments = self.binary_search_segments(video_path, question)
        
        if not relevant_segments:
            logger.warning("æœªæ‰¾åˆ°ç›¸å…³ç‰‡æ®µ")
            return []
        
        # æå–å…³é”®å¸§
        extracted_frames = []
        
        for i, (start_time, end_time, relevance) in enumerate(relevant_segments):
            # é€‰æ‹©ç‰‡æ®µä¸­é—´çš„æ—¶é—´ç‚¹ä½œä¸ºå…³é”®å¸§
            key_timestamp = (start_time + end_time) / 2
            
            try:
                # æå–å¸§
                frame = self.extract_frame_at_time(video_path, key_timestamp)
                
                # ä¿å­˜å¸§
                frame_filename = f"frame_{i+1}_{key_timestamp:.1f}s.jpg"
                frame_path = os.path.join(output_dir, frame_filename)
                
                # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
                pil_image = Image.fromarray(frame)
                pil_image.save(frame_path, "JPEG", quality=95)
                
                # è·å–å¸§æè¿°
                frame_desc = self.clip_analyzer.describe_frame(frame, self.deepseek_client)
                
                frame_info = {
                    "frame_id": i + 1,
                    "timestamp": key_timestamp,
                    "segment_start": start_time,
                    "segment_end": end_time,
                    "relevance_score": relevance,
                    "description": frame_desc,
                    "filename": frame_filename,
                    "file_path": frame_path
                }
                
                extracted_frames.append(frame_info)
                
                logger.info(f"âœ“ æå–å…³é”®å¸§ {i+1}: {key_timestamp:.1f}s (ç›¸å…³æ€§: {relevance:.3f})")
                
            except Exception as e:
                logger.error(f"æå–å¸§å¤±è´¥: {e}")
                continue
        
        return extracted_frames
    
    def process_video_with_questions(self, video_path: str, output_base_dir: str) -> Dict:
        """ä½¿ç”¨æ‰€æœ‰é¢„å®šä¹‰é—®é¢˜å¤„ç†è§†é¢‘"""
        video_name = Path(video_path).stem
        video_output_dir = os.path.join(output_base_dir, video_name)
        
        logger.info(f"=" * 60)
        logger.info(f"å¤„ç†è§†é¢‘: {video_name}")
        logger.info(f"è¾“å‡ºç›®å½•: {video_output_dir}")
        
        results = {
            "video_name": video_name,
            "video_path": video_path,
            "output_directory": video_output_dir,
            "questions_results": []
        }
        
        for i, question in enumerate(self.predefined_questions):
            logger.info(f"\n--- é—®é¢˜ {i+1}/{len(self.predefined_questions)} ---")
            
            # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºå­ç›®å½•
            question_dir = os.path.join(video_output_dir, f"question_{i+1}")
            
            # æå–å…³é”®å¸§
            extracted_frames = self.extract_key_frames(video_path, question, question_dir)
            
            question_result = {
                "question_id": i + 1,
                "question": question,
                "output_directory": question_dir,
                "extracted_frames": extracted_frames,
                "frame_count": len(extracted_frames)
            }
            
            results["questions_results"].append(question_result)
            
            logger.info(f"é—®é¢˜ {i+1} å®Œæˆï¼Œæå–äº† {len(extracted_frames)} ä¸ªå…³é”®å¸§")
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        results_file = os.path.join(video_output_dir, "extraction_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        return results

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== äºŒåˆ†æ³•è§†é¢‘å…³é”®å¸§æå–å™¨ ===")
    
    # é…ç½®è·¯å¾„
    video_dir = "dataset/video"
    output_dir = "extracted_frames"
    
    # æ£€æŸ¥è§†é¢‘ç›®å½•
    if not os.path.exists(video_dir):
        logger.error(f"è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {video_dir}")
        logger.info("è¯·åˆ›å»ºç›®å½•å¹¶æ”¾å…¥è§†é¢‘æ–‡ä»¶:")
        logger.info(f"mkdir -p {video_dir}")
        return
    
    # è·å–DeepSeek APIå¯†é’¥
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    if not deepseek_api_key:
        logger.warning("æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = BinarySearchExtractor(deepseek_api_key)
    
    # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(Path(video_dir).glob(f"*{ext}"))
        video_files.extend(Path(video_dir).glob(f"*{ext.upper()}"))
    
    if not video_files:
        logger.error(f"åœ¨ {video_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        logger.info(f"æ”¯æŒçš„æ ¼å¼: {', '.join(video_extensions)}")
        return
    
    logger.info(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªè§†é¢‘
    all_results = []
    
    for video_file in video_files:
        try:
            result = extractor.process_video_with_questions(str(video_file), output_dir)
            all_results.append(result)
            
        except Exception as e:
            logger.error(f"å¤„ç†è§†é¢‘ {video_file} å¤±è´¥: {e}")
            continue
    
    # ä¿å­˜æ€»ä½“ç»“æœ
    summary_file = os.path.join(output_dir, "extraction_summary.json")
    summary = {
        "total_videos": len(video_files),
        "processed_videos": len(all_results),
        "predefined_questions": extractor.predefined_questions,
        "results": all_results
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info("=" * 60)
    logger.info("å¤„ç†å®Œæˆ!")
    logger.info(f"æ€»è§†é¢‘æ•°: {len(video_files)}")
    logger.info(f"æˆåŠŸå¤„ç†: {len(all_results)}")
    logger.info(f"ç»“æœæ‘˜è¦: {summary_file}")
    
    # æ‰“å°æå–ç»Ÿè®¡
    total_frames = sum(
        sum(q["frame_count"] for q in result["questions_results"]) 
        for result in all_results
    )
    logger.info(f"æ€»æå–å¸§æ•°: {total_frames}")

if __name__ == "__main__":
    main() 