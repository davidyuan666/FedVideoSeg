# FedVideoQA Configuration
project:
  name: "FedVideoQA"
  version: "1.0.0"
  description: "Federated Learning Framework for Video Question Answering"

# API Configuration
deepseek:
  api_url: "https://api.deepseek.com/v1"
  model: "deepseek-chat"
  max_tokens: 4096
  temperature: 0.7

# Video Processing
video:
  supported_formats: [".mp4", ".avi", ".mov", ".mkv", ".webm"]
  max_duration: 3600  # seconds
  segment_min_length: 5  # seconds
  segment_max_length: 30  # seconds
  frame_rate: 1  # frames per second for analysis

# Multimodal Models
models:
  clip:
    model_name: "ViT-B/32"
    device: "cuda"
  whisper:
    model_name: "base"
    device: "cuda"
  bert:
    model_name: "bert-base-uncased"
    device: "cuda"

# Federated Learning
federated:
  num_rounds: 100
  num_clients: 8
  fraction_fit: 0.8
  fraction_evaluate: 0.5
  min_fit_clients: 3
  min_evaluate_clients: 3
  min_available_clients: 3

# Privacy Settings
privacy:
  epsilon: 2.0  # Differential privacy budget
  delta: 1e-5
  max_grad_norm: 1.0
  noise_multiplier: 1.1
  gradient_sparsification_threshold: 0.01
  max_communication_size_mb: 100

# Training Parameters
training:
  batch_size: 16
  learning_rate: 1e-4
  weight_decay: 1e-5
  num_epochs: 10
  loss_type: "mse"  # æˆ– "cosine"
  warmup_steps: 100
  gradient_accumulation_steps: 4

# Attention Mechanism
attention:
  alpha_v: 0.4  # Visual attention weight
  alpha_a: 0.3  # Audio attention weight
  alpha_t: 0.3  # Text attention weight
  hidden_dim: 512
  num_heads: 8

# Performance Thresholds
thresholds:
  qa_accuracy: 0.85
  response_latency_ms: 60
  privacy_budget_per_round: 0.02 